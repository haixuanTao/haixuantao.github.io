## ONNX Server: Serving BERT as an API
[![GitHub stars](https://img.shields.io/github/stars/haixuanTao/bert-onnx-rs-server?style=social&label=Star&maxAge=2592000)](https://github.com/haixuanTao/bert-onnx-rs-server/)
[![GitHub forks](https://img.shields.io/github/forks/haixuanTao/bert-onnx-rs-server?style=social&label=Fork&maxAge=2592000)](https://github.com/haixuanTao/bert-onnx-rs-server/)
[![GitHub stars](https://img.shields.io/github/last-commit/haixuantao/bert-onnx-rs-server)](https://github.com/haixuanTao/bert-onnx-rs-server/)

Let say you want to serve a BERT-like model through a server API endpoint.

On my setup, I got those metrics:

| |Python FastAPI |Rust Actix Web |Speedup |
| --- | --- | --- | --- |
|Encoding time |400Œºs |100Œºs | |
|ONNX Inference time |~10ms |~10ms | |
|API overhead time |~2ms |~1ms | |
|Mean Latency |12.8ms |10.4ms |-20%‚è∞ |
|Requests/secs |77.5 #/s |95 #/s |\+22%üçæ |

**The gain in performance comes from moving from considered ‚ÄúFast‚Äù Python library to Rust: FastAPI -> Actix Web, BertokenizerFast -> Rust Tokenizer.**

**Thus, as Rust libraries tend to be faster than Python ones, the more functionalities you will have, the more speedup you‚Äôre going to see with Rust when serving Deep Learning.**

**That‚Äôs why, for performance-centric Deep Learning applications such as Real-Time Deep Learning, Embedded Deep Learning, Large-Scale AI servers ‚Ä¶. I can definitely see Rust be a good fit!** ‚ù§Ô∏è‚Äçü¶Ä

_Git:_ [_https://github.com/haixuanTao/bert-onnx-rs-server_](https://github.com/haixuanTao/bert-onnx-rs-server)
