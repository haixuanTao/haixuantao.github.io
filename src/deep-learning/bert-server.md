## ONNX Server: Serving BERT as an API

Another use case is serving a BERT-like model as a server with a REST endpoint.

To see if Rust could be more performant than Python, I served the onnx model through [actix-web](https://actix.rs/), and to benchmark it, I made a clone in Python with [FastAPI](https://fastapi.tiangolo.com/).



### Performance 

For a request of one phrase:

| |Python FastAPI |Rust Actix Web |Speedup |
| --- | --- | --- | --- |
|Encoding |400Œºs |100Œºs | |
|ONNX Inference |~10ms |~10ms | |
|API overhead |~2ms |~1ms | |
|Mean Latency |12.8ms |10.4ms |-20%‚è∞ |
|Requests/secs |77.5 #/s |95 #/s |\+22%üî• |

The gain in performance comes from moving from considered ‚ÄúFast‚Äù Python library to Rust: 
- FastAPI ‚è© Actix Web
- BertokenizerFast Ô∏è‚è© Rust Tokenizer

Thus, as Rust libraries tend to be faster than Python ones, Rust will be faster when the application is a composition of libraries.

That‚Äôs why, I can see Rust be a good fit for excessively performance centric applications such as Real-Time Deep Learning, Embedded Deep Learning, Large-Scale AI servers! ‚ù§Ô∏è‚Äçü¶Ä

_Check the code:_ [_https://github.com/haixuanTao/bert-onnx-rs-server_](https://github.com/haixuanTao/bert-onnx-rs-server)  
[<img alt="github" src="https://img.shields.io/badge/bert--onnx--rs--server-fff?labelColor=000&logo=github" height="20">](https://github.com/haixuantao/bert-onnx-rs-server)
[![GitHub stars](https://img.shields.io/github/stars/haixuanTao/bert-onnx-rs-server?style=social&label=Star&maxAge=2592000)](https://github.com/haixuanTao/bert-onnx-rs-server/)