# preprocessing

### 2. Preprocessing time

| |Tokenizing time per phrase |Speedup |
| --- | --- | --- |
|Python BertTokenizer |1000Î¼s | |
|Python BertTokenizerFast |200-600Î¼s |**x2.5**ðŸ”¥ |
|Rust [Tokenizer](https://docs.rs/tokenizers/0.10.1/tokenizers/) |50-150Î¼s |**x4**ðŸ”¥ |

**Gains can be made on DL Preprocessing! You can tokenize 4 times faster in Rust compared to Python, with the same Hugging Face Tokenizer library.**
